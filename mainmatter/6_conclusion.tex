\chapter{General Conclusion}\label{sec:conclusion}

\section{Conclusion}
Throughout this thesis, the issues of motion and depth for scene analysis were studied. In particular, event cameras of both low-, mid-, and high-resolution were used for solving both problems, and the introduction of a LiDAR sensor was also critical in the case of the depth estimation.

In the case of motion estimation, a real-time optical flow method was proposed, relying on a single event camera. A pipeline-based computation was proposed, for allowing parallelism of the tasks. In particular, a 4-step method was proposed here, for temporally accumulating events as binary edge images, denoising and correcting them, converting them in a dense frame-based representation, and computing the final optical flow using a state-of-the-art frame-based method.

In the case of depth estimation, a convolutional-based network was first proposed, relying on both high-rate data from a single event camera, and on lower-rate point clouds from a single LiDAR sensor. This network is composed of two separate encoding branches for processing both inputs asynchronously, of central memory states for recurrence and for fusing the two modalities, and of a single decoding branch for estimating the final dense depth maps. A notion of ``two depths per event'' was also proposed, for taking into account the change-based nature of events, and a simulated dataset was recorded and released for improving the training and evaluation of our network.

Finally, an attention-based network was also proposed, for improving the fusion of the LiDAR and event data. This network is composed of two separate encoding branches for processing both inputs, of a propagation memory for temporally upsampling the LiDAR data, a central cross-attention module for fusing the two modalities, a single central memory for recurrence, and a decoding branch for estimating the final dense depth maps.

Both theoretical and practical analysis were conducted to demonstrate the relevance of the contributions. For optical flow, accuracies close to the state of the art were obtained, but for much faster computation times (250Hz and 83Hz for low- and high-resolution inputs respectively) and with a low latency (10ms and 27ms for low- and high-resolution inputs respectively). For the depth estimation, the convolutional-based network set first a new state-of-the-art reference, with an error reduction of up to 61\% compared to the previous LiDAR-and-event state-of-the-art. The attention-based network then refined these results, by allowing for an error reduction for close ranges, dividing the errors of the convolutional network up to four times.

As a general conclusion, we have seen throughout this thesis that motion and depth estimation are critical components for scene analysis. By nature, the event camera constitutes a fitting sensor for these problems, due to its innate motion-related encoding of data. The LiDAR sensor also complements surprisingly well the event camera for a multimodal depth estimation. While the robotic field was considered here, as being the one where scene analysis is the most important and where the combination of event cameras and LiDARs is the most likely to be used, we believe that our research could be extended to other fields, from biology to industrial automation.


\section{Contributions}
Across this thesis work, several major contributions were made, which we summarize here.

\paragraph{Optical Flow (RTEF)}
The first and main contribution of the work on event-based optical flow was proposing a real-time method. While some works had already been done in that sense~\cite{ParedesValls2021BackTE,Akolkar2020SeeBY}, they lacked accuracy, and hardly scaled to a high-resolution input. On the contrary, our work was directed from the start with the multi-resolution aspect in mind, and our method is therefore able of a good accuracy for both low-, mid-, and high-resolution event cameras. Another contribution of this work was our ``negated exponential distance transform'', which was especially critical for improving the accuracy results by giving us the ability to use a proven frame-based optical flow method. While only used in the context of optical flow estimation here, we believe that this dense representation could also provide interesting results for other applications where real-time is required. Finally, a high-speed high-definition event-based indoor dataset was recorded and shared, for allowing the evaluation of our optical flow method on complex and fast motions, a feature which is missing from most state-of-the-art datasets.

\paragraph{Depth Estimation (ALED)}
Regarding the work on depth estimation using a convolutional network, the very first contribution was the idea itself of combining a LiDAR and an event camera for estimating dense depth maps, which had not been explored previously in the literature. As showed throughout \cref{sec:aled,sec:delta}, despite their apparent incompatibilities (different type of data and of sparsity, vastly different output rates), these two sensors actually complement each other perfectly in the context of this problem. Another fundamental contribution of this work was the idea of associating two depths to each event, which is the only method that takes into account the change-based nature of events while still allowing for their reprojection in the 3D world. One of the main contribution of this work also lies in the \acrshort{aled} network itself. While originally inspired by the RAMNet architecture of Gehrig \textit{et al.}~\cite{Gehrig2021CombiningEA}, \acrshort{aled} is refined in several ways (more encoding scales, the use of convex upsampling during decoding), largely improving the accuracy of the depth maps. Finally, the recording and sharing of the simulated \acrshort{sled} dataset is also a major contribution, for allowing the training and evaluation of depth estimation methods with dense ground truth data, a feature that is missing from the state-of-the-art real-world datasets. \acrshort{sled} was also made in a modular way, meaning that it could be extended for introducing further tasks (optical flow, semantic segmentation, object detection and recognition, \dots).

\paragraph{Depth Estimation (DELTA)}
As for the second work on depth estimation, our first contribution is on the concept itself of using an attention-based network for fusing LiDAR and event data. Even if they were not successful, we believe that our tests on a fully sparse version of this network are valuable, as they allowed for a better understanding of the Transformer and the attention mechanism, their limitations, and as they highlight the current need for a dense, patched-based representation. Of course, our main contribution here is the \acrshort{delta} network itself, which is a fully novel recurrent network architecture relying on self- and cross-attention modules. As shown in the ablation study of \cref{sec:delta}, \cref{sec:delta:eval:ablation}, the most critical contributions are the propagation memory, allowing for a better temporal upsampling of the LiDAR data, the central memory and its GRU-based update, allowing for the introduction of recurrence, solving the cases where few events are available and offering more stability overall, and the central cross-attention module, for modeling the interactions between the LiDAR and event data.

\paragraph{Open Science}
A significant emphasis during these three years has also been put on ensuring that our work was as accessible and as exploitable by the community as possible. All preprints of the published articles were made available on arXiv and/or HAL. Dedicated project pages were also created, for giving a quick overview of each work:
\begin{itemize}\small
  \item \url{https://vbrebion.github.io/RTEF/}
  \item \url{https://vbrebion.github.io/ALED/}
\end{itemize}
All the source codes of the published methods were open-sourced, published in a clean version, and properly documented:
\begin{itemize}\small
  \item \url{https://github.com/heudiasyc/rt_of_low_high_res_event_cameras}
  \item \url{https://github.com/heudiasyc/ALED}
  \item \url{https://github.com/heudiasyc/SLED}
\end{itemize}
Finally, datasets were also made publicly available, through the platform of the laboratory:
\begin{itemize}\small
  \item \url{https://datasets.hds.utc.fr/project/7}
  \item \url{https://datasets.hds.utc.fr/project/9}
\end{itemize}

\paragraph{Community Source Codes}
Several contributions were also made to improve other source codes and repositories of the community, and are briefly listed here:
\begin{itemize}\small
  \item \url{https://github.com/astuff/avt_vimba_camera/pull/22}
  \item \url{https://github.com/prophesee-ai/prophesee_ros_wrapper/pull/29}
  \item \url{https://github.com/uzh-rpg/rpg_dvs_ros/pull/111}
  \item \url{https://github.com/uzh-rpg/event-based_vision_resources/pull/150}
  \item \url{https://github.com/uzh-rpg/event-based_vision_resources/pull/213}
  \item \url{https://github.com/carla-simulator/carla/issues/5367}
  \item \url{https://github.com/carla-simulator/carla/issues/5732}
  \item \url{https://github.com/carla-simulator/carla/issues/6103}
  \item \url{https://github.com/carla-simulator/carla/issues/6552}
  \item \url{https://github.com/AlbertoSabater/EventTransformerPlus/issues/1}
\end{itemize}

\section{Discussions and Perspectives}\label{sec:conclusion:perspectives}
As discussed throughout the chapters of this thesis, several improvements, reworks, and future works could be proposed to improve the contributions proposed as part of this thesis.

Regarding our optical flow method first, \acrshort{rtef}, one of the main limits to highlight is its inability to produce accurate results in case of large motions over short periods of time. This limitation mainly comes from our use of a frame-based approach, which discards the timestamps. One solution is to reduce the accumulation time for these cases, but it implies disregarding the real-time constraint. By construction, learning-based and contrast-maximization-based methods perform better on these cases, but their real-time compatibility remains an open question. Similarly, while our method had state-of-the-art performances when it was published in 2021~\cite{Brebion2022RealTimeOF}, optical flow has since become one of the most researched subjects in event-based vision, and learning-based and contrast-maximization-based methods have particularly risen as clear favorites, with impressive accuracy. Finally, a theoretical limitation that was highlighted in the concluding remarks of \cref{sec:ebof} was on the intrinsic sense of computing optical flow on events the way we did, that is, computing motion of changes themselves. This remains an open question, and we believe that an in-depth theoretical analysis on this issue may be required for making sure that event cameras are exploited correctly.

Regarding our two depth estimation methods, \acrshort{aled} and \acrshort{delta}, both could also be improved in several ways. As discussed, the choice of the Event Volume as the input representation for the events could be revised. While it was chosen at the time as a compact representation of the event data that still kept a nearly maximal amount of information, more recent works~\cite{Zubic2023FromCC} have shown that having a learned representation could also be beneficial, by only keeping the required data in an even more compact form factor. Both our convolutional and attention-based networks are also not real-time compatible. The emphasis was never set on this specific point during these works, as we observed through our work on optical flow that this constraint limited our final accuracy, and that it should not be a priority considering that we were among the firsts to examine the fusion of the LiDAR and event modalities. However, for an implementation on a real robotic system, some revisions would have to be brought to our networks, in an effort to make them faster while retaining most of their accuracy. This could be done through quantized low-precision methods for instance, like 4-bit networks~\cite{Trusov2020FastIO,Dettmers2023QLoRAEF}. Despite our best efforts, we were also unable to make the fully sparse version of the attention-based network produce adequate results. We believe that one of the biggest limitations was the small size of these networks, which limited their learning ability on such a complex task. With the popularity of the Transformer architecture, however, more optimized implementations of the attention calculation process are starting to be integrated in popular libraries, which could be of great help in the future for building bigger attention-based networks with less memory restrictions. Finally, we believe that a real-world dataset with dense ground truth depth maps would be of great help for training and evaluating more accurately the current and future event-based depth estimation methods. Several sub-works in that direction were conducted during the thesis, as described in \cref{sec:appendix:add_exp}, but several technical issues and an overall lack of time ultimately led us to the recording of the simulated \acrshort{sled} dataset.

More generally, as discussed earlier, one of the main extensions of this work could be on 3D scene flow, which could be obtained in our case by combining both the 2D optical flow data and the depth estimations. Scene flow is slowly rising and becoming an important issue in computer vision, but it remains to this day a rather unexplored issue with event cameras~\cite{Carneiro2014AsynchronousE3,Ieng2017EventBased3M}, and could therefore constitute an interesting subject to explore.

Further work could also be conducted on the interactions with the ``planning'' and ``control'' modules of the robotic pipeline shown in \cpageref{fig:intro:robotic_pipeline} of this thesis. The work conducted during these three years was purely centered on a perception aspect, but being able to apply the optical flow and depth estimation methods on a real robot would probably pose interesting questions as to how they could be used further down the pipeline, and as to the level of accuracy needed for accomplishing desired tasks.

As a more general conclusion to these perspectives, we believe that, while the event camera is an intriguing and fascinating sensor, large amounts of work remain to be done to level them with their traditional frame-based counterparts, and to exploit their unique properties to their best. As such, this thesis constitutes only a small contribution towards that goal, but we hope that both our theoretical and technical works will be able to guide and inspire other authors.
